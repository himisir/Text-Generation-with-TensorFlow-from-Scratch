{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2124,"sourceType":"datasetVersion","datasetId":1028},{"sourceId":32267,"sourceType":"datasetVersion","datasetId":24984},{"sourceId":38997,"sourceType":"datasetVersion","datasetId":30569},{"sourceId":1386132,"sourceType":"datasetVersion","datasetId":809083},{"sourceId":6432870,"sourceType":"datasetVersion","datasetId":1977878}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"border: 2px solid #28a745; border-radius: 15px; padding: 10px;\">\n        <h3 align=\"center\" style=\"color: #28a745;\">Creating a Text Generator from Scratch: A Step-by-Step Tutorial</h3>\n    </div>\nLearn how to build a text generator from scratch. I'll show you how to prepare your data, tokenize it, and set it up for text generation. Plus, I'll give you tips on saving memory and building your model. You'll be generating text in no time!\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h3 align=\"center\">Importing Necessary Libraries</h3>\n</div>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To make use of all existing GPUs, we are using mirror strategy.\nFor more information about GPU training, please visit this discussion [link](https://www.kaggle.com/discussions/getting-started/435499)","metadata":{}},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n# strategy = tf.distribute.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:48.673609Z","iopub.execute_input":"2023-09-07T09:54:48.674497Z","iopub.status.idle":"2023-09-07T09:54:52.018311Z","shell.execute_reply.started":"2023-09-07T09:54:48.674456Z","shell.execute_reply":"2023-09-07T09:54:52.017218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h1 align=\"center\">Dataset Preparation and Preprocessing</h1>\n</div>\nWe'll begin by reading the data from the '/kaggle/input' directory and then selecting only the lines related to players, as that's what we want to train our model on. After this initial filtering, we need to process the dataset by converting it to lowercase for better machine handling.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/shakespeare-plays/Shakespeare_data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:52.020322Z","iopub.execute_input":"2023-09-07T09:54:52.020803Z","iopub.status.idle":"2023-09-07T09:54:52.351231Z","shell.execute_reply.started":"2023-09-07T09:54:52.020766Z","shell.execute_reply":"2023-09-07T09:54:52.350036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's take a peek at the current state of our dataset.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:52.354328Z","iopub.execute_input":"2023-09-07T09:54:52.354797Z","iopub.status.idle":"2023-09-07T09:54:52.383252Z","shell.execute_reply.started":"2023-09-07T09:54:52.354760Z","shell.execute_reply":"2023-09-07T09:54:52.382087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Irrelevant Columns\nAs you can see, the current dataset contains information that is not useful for a language model. Our primary focus here is training our model on conversations/text to generate new text based on it, and the 'Playerline' column provides the relevant data. Therefore, we'll create our base dataset based on that.","metadata":{}},{"cell_type":"code","source":"dataset = data['PlayerLine']","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:52.384491Z","iopub.execute_input":"2023-09-07T09:54:52.384850Z","iopub.status.idle":"2023-09-07T09:54:52.394317Z","shell.execute_reply.started":"2023-09-07T09:54:52.384810Z","shell.execute_reply":"2023-09-07T09:54:52.393051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We'll also convert these lines to lowercase to ensure consistency.","metadata":{}},{"cell_type":"code","source":"corpus = []\nwith strategy.scope():\n    for line in dataset:\n        lowercase_line = line.lower()\n        corpus.append(lowercase_line)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:52.396726Z","iopub.execute_input":"2023-09-07T09:54:52.397082Z","iopub.status.idle":"2023-09-07T09:54:52.475937Z","shell.execute_reply.started":"2023-09-07T09:54:52.397056Z","shell.execute_reply":"2023-09-07T09:54:52.474686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now, let's take a look at what we're working with.","metadata":{}},{"cell_type":"code","source":"corpus[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:52.479411Z","iopub.execute_input":"2023-09-07T09:54:52.480158Z","iopub.status.idle":"2023-09-07T09:54:52.487651Z","shell.execute_reply.started":"2023-09-07T09:54:52.480118Z","shell.execute_reply":"2023-09-07T09:54:52.486431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h1 align=\"center\">Initialize Tokenizer</h1>\n</div>\nNext, we'll initialize the tokenizer and fit our processed data with it. The tokenizer assigns a numerical value to each word. Machine learning operations are essentially matrix operations, so we need to represent our data in a way that's suitable for matrix operations.","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:52.489603Z","iopub.execute_input":"2023-09-07T09:54:52.490123Z","iopub.status.idle":"2023-09-07T09:54:54.462785Z","shell.execute_reply.started":"2023-09-07T09:54:52.490082Z","shell.execute_reply":"2023-09-07T09:54:54.461613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_token = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:54.464339Z","iopub.execute_input":"2023-09-07T09:54:54.465293Z","iopub.status.idle":"2023-09-07T09:54:54.471367Z","shell.execute_reply.started":"2023-09-07T09:54:54.465255Z","shell.execute_reply":"2023-09-07T09:54:54.470052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize what the tokenized data looks like.","metadata":{}},{"cell_type":"code","source":"def key_pair(num):    \n    count=0\n    for key, value in word_to_token.items():\n        if count>=num: break\n        print(f''''{key:}': {value},''')\n        count +=1\nkey_pair(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:54.477825Z","iopub.execute_input":"2023-09-07T09:54:54.478120Z","iopub.status.idle":"2023-09-07T09:54:54.487625Z","shell.execute_reply.started":"2023-09-07T09:54:54.478094Z","shell.execute_reply":"2023-09-07T09:54:54.486158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, you might wonder how many word-key pairs we have. This section will provide that information.","metadata":{}},{"cell_type":"code","source":"total_words = len(word_to_token)+1\nprint(total_words)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:54.489713Z","iopub.execute_input":"2023-09-07T09:54:54.490176Z","iopub.status.idle":"2023-09-07T09:54:54.501695Z","shell.execute_reply.started":"2023-09-07T09:54:54.490140Z","shell.execute_reply":"2023-09-07T09:54:54.500469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h1 align=\"center\">Sequence-to-N-gram</h1>\n</div>\nSequencing involves converting sentences into numerical values based on the word-key pairs we created earlier. After this step, we'll perform N-gram processing, a crucial step for text generation.\n\nIn machine learning, our goal is to predict outcomes based on the provided data. For this task, we aim to predict the next word given the first few words of a sentence. Thus, we need to prepare our data accordingly.\n\nWhat we'll do here is convert each sentence into an n-gram format. This means breaking it down into sequences, with the last word as the target.\n \nSource data : I love Artificial Intelligence\n\n``X---------------------------------y\nI                                 love\nI love                            Artificial\nI love Artificial                 Intelligence\n``\n\nIn essence, we start with the first word of a sentence as the feature, and the next word becomes the label. We repeat this process until the end. So, now our model has a dataset that tells it, given a sequence, what the next word should be.\n\nLater, we'll add padding to ensure that each feature-label pair has the same size.","metadata":{}},{"cell_type":"code","source":"input_sequences = []\nwith strategy.scope():\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:54.503611Z","iopub.execute_input":"2023-09-07T09:54:54.504037Z","iopub.status.idle":"2023-09-07T09:54:57.667566Z","shell.execute_reply.started":"2023-09-07T09:54:54.503986Z","shell.execute_reply":"2023-09-07T09:54:57.666539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How many sequences do we have after sequencing?","metadata":{}},{"cell_type":"code","source":"len(input_sequences)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:57.669028Z","iopub.execute_input":"2023-09-07T09:54:57.669489Z","iopub.status.idle":"2023-09-07T09:54:57.677445Z","shell.execute_reply.started":"2023-09-07T09:54:57.669450Z","shell.execute_reply":"2023-09-07T09:54:57.676265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How sentences look like after sequencing?","metadata":{}},{"cell_type":"code","source":"input_sequences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:57.678908Z","iopub.execute_input":"2023-09-07T09:54:57.679330Z","iopub.status.idle":"2023-09-07T09:54:57.693311Z","shell.execute_reply.started":"2023-09-07T09:54:57.679292Z","shell.execute_reply":"2023-09-07T09:54:57.692151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we proceed further, let's take a sample for future comparison.","metadata":{}},{"cell_type":"code","source":"before = input_sequences[1]","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:57.695093Z","iopub.execute_input":"2023-09-07T09:54:57.695543Z","iopub.status.idle":"2023-09-07T09:54:57.702351Z","shell.execute_reply.started":"2023-09-07T09:54:57.695499Z","shell.execute_reply":"2023-09-07T09:54:57.701484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also need to determine the length of the longest sequence, which we'll use later in this project.","metadata":{}},{"cell_type":"code","source":"max_seq_len = max(len(x) for x in input_sequences)\nprint(max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:57.704064Z","iopub.execute_input":"2023-09-07T09:54:57.704471Z","iopub.status.idle":"2023-09-07T09:54:57.778747Z","shell.execute_reply.started":"2023-09-07T09:54:57.704438Z","shell.execute_reply":"2023-09-07T09:54:57.777752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Padding\nNow, the issue with the n-grams we've created is that not all entries have the same length. When dealing with matrix manipulation, consistency in size and shape is crucial. To address this, we'll add padding with 'zero' values to each sequence to make them the same size. This is where our maximum sequence length comes in handy.\n\nYou might wonder why we're adding zeros before the original sequence. I'll answer that in a few cells.","metadata":{}},{"cell_type":"code","source":"input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding = 'pre'))","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:54:57.780120Z","iopub.execute_input":"2023-09-07T09:54:57.780515Z","iopub.status.idle":"2023-09-07T09:55:01.082889Z","shell.execute_reply.started":"2023-09-07T09:54:57.780481Z","shell.execute_reply":"2023-09-07T09:55:01.081898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"after = input_sequences[1]","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:01.084631Z","iopub.execute_input":"2023-09-07T09:55:01.085050Z","iopub.status.idle":"2023-09-07T09:55:01.091358Z","shell.execute_reply.started":"2023-09-07T09:55:01.085013Z","shell.execute_reply":"2023-09-07T09:55:01.089135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's compare the before-and-after pictures of sequencing.","metadata":{}},{"cell_type":"code","source":"print(f'Before: {before}')\nprint(f'After: {after}')","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:01.093061Z","iopub.execute_input":"2023-09-07T09:55:01.093406Z","iopub.status.idle":"2023-09-07T09:55:01.106673Z","shell.execute_reply.started":"2023-09-07T09:55:01.093378Z","shell.execute_reply":"2023-09-07T09:55:01.105248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, we've added zeros in front of the original sequences (e.g., 135, 3). We've done this to ensure that every entry in our dataset is of the same length.","metadata":{}},{"cell_type":"markdown","source":"### Creating Feature-Label Pairs\nNow that we've prepared our corpus, we can focus on creating the actual dataset, consisting of feature-label pairs. We'll need this dataset to train our model to generate text.\n\nRemember the question about why we're adding padding in front of the original sequence? It's because we're going to construct the training set by making the last value of a sequence the label and the rest of the sequence the feature. We're training the model to predict which word typically follows certain structures. Once the training is complete, the model should be able to generate text.","metadata":{}},{"cell_type":"code","source":"features, labels = input_sequences[:, :-1], input_sequences[:, -1],","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:01.108694Z","iopub.execute_input":"2023-09-07T09:55:01.109103Z","iopub.status.idle":"2023-09-07T09:55:01.119390Z","shell.execute_reply.started":"2023-09-07T09:55:01.109073Z","shell.execute_reply":"2023-09-07T09:55:01.118165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To categorical and beyond!\nWe'll convert the labels to categorical values.","metadata":{}},{"cell_type":"code","source":"labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:01.120995Z","iopub.execute_input":"2023-09-07T09:55:01.121377Z","iopub.status.idle":"2023-09-07T09:55:02.958863Z","shell.execute_reply.started":"2023-09-07T09:55:01.121347Z","shell.execute_reply":"2023-09-07T09:55:02.957685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Out Of Memory Problem\nUnfortunately, after the n-gram conversion, the dataset becomes extremely large, making it challenging to run on Kaggle's basic resources. To mitigate this, we'll select a tiny portion of the data for our training, approximately 0.05% of the total sequences.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    n = 0.05 # We are only taking a chunk of this huge dataset to fit it on the RAM\n    slice_size = int(len(features)*n)\n    np.save('/kaggle/working/features', features[:slice_size, :])\n    np.save('/kaggle/working/labels', labels[:slice_size, :])","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:02.960430Z","iopub.execute_input":"2023-09-07T09:55:02.961509Z","iopub.status.idle":"2023-09-07T09:55:14.097213Z","shell.execute_reply.started":"2023-09-07T09:55:02.961467Z","shell.execute_reply":"2023-09-07T09:55:14.096214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's another trick I've learned to address Kaggle's out-of-memory (OOM) issues. We save the dataset and load it when needed, instead of going through the preprocessing steps again. Loading a saved dataset is more efficient and helps with managing RAM.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    features = np.load('/kaggle/working/features.npy')\n    labels = np.load('/kaggle/working/labels.npy')","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:14.098740Z","iopub.execute_input":"2023-09-07T09:55:14.099333Z","iopub.status.idle":"2023-09-07T09:55:30.753410Z","shell.execute_reply.started":"2023-09-07T09:55:14.099297Z","shell.execute_reply":"2023-09-07T09:55:30.748352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h1 align=\"center\">Model Design</h1>\n</div>\nThis is the model that will learn how to generate text. It takes sequences of words, embeds them into vectors, processes them with Bidirectional LSTMs, and predicts the next word using a softmax output layer.","metadata":{}},{"cell_type":"code","source":"#Create the model!\ndef generator_model():\n    tf.random.set_seed(42)\n    model = Sequential()\n    model.add(Embedding(total_words, 100, input_length = max_seq_len-1)),\n    model.add(Bidirectional(LSTM(64, return_sequences = True))),\n    model.add(Bidirectional(LSTM(32))),\n    model.add(Dense(64, activation = 'relu')),\n    model.add(Dense(total_words, activation = 'softmax'))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:30.759109Z","iopub.execute_input":"2023-09-07T09:55:30.759545Z","iopub.status.idle":"2023-09-07T09:55:30.767221Z","shell.execute_reply.started":"2023-09-07T09:55:30.759505Z","shell.execute_reply":"2023-09-07T09:55:30.765939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compliling the Model","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = generator_model()\n    model.compile(loss = 'categorical_crossentropy', \n                 optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002),\n                 metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:30.768801Z","iopub.execute_input":"2023-09-07T09:55:30.769440Z","iopub.status.idle":"2023-09-07T09:55:32.622661Z","shell.execute_reply.started":"2023-09-07T09:55:30.769405Z","shell.execute_reply":"2023-09-07T09:55:32.621528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Summary","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-07T09:55:32.624166Z","iopub.execute_input":"2023-09-07T09:55:32.624559Z","iopub.status.idle":"2023-09-07T09:55:32.648698Z","shell.execute_reply.started":"2023-09-07T09:55:32.624519Z","shell.execute_reply":"2023-09-07T09:55:32.647965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainging the Model","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nhistory = model.fit(features, labels, epochs = EPOCHS)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-09-07T09:55:32.649751Z","iopub.execute_input":"2023-09-07T09:55:32.650107Z","iopub.status.idle":"2023-09-07T10:05:14.510247Z","shell.execute_reply.started":"2023-09-07T09:55:32.650073Z","shell.execute_reply":"2023-09-07T10:05:14.509115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h1 align=\"center\">Loss Accuracy Curve</h1>\n</div>\nLet's take a look the training progress","metadata":{}},{"cell_type":"code","source":"#Helper function\ndef plot_graph(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-07T10:05:14.516692Z","iopub.execute_input":"2023-09-07T10:05:14.516999Z","iopub.status.idle":"2023-09-07T10:05:14.524916Z","shell.execute_reply.started":"2023-09-07T10:05:14.516972Z","shell.execute_reply":"2023-09-07T10:05:14.523732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graph(history, 'accuracy')\nplot_graph(history, 'loss')","metadata":{"execution":{"iopub.status.busy":"2023-09-07T10:05:14.526388Z","iopub.execute_input":"2023-09-07T10:05:14.526898Z","iopub.status.idle":"2023-09-07T10:05:15.147826Z","shell.execute_reply.started":"2023-09-07T10:05:14.526864Z","shell.execute_reply":"2023-09-07T10:05:15.146865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the Model\nWe're saving the model and loading it in case we want to skip the training process.","metadata":{}},{"cell_type":"code","source":"model.save('/kaggle/working/test_generator.h5')\n#load the save model if you want to skip the training\n#tf.keras.models.load_model('/kaggle/working/test_generator.h5') ","metadata":{"execution":{"iopub.status.busy":"2023-09-07T10:05:15.151019Z","iopub.execute_input":"2023-09-07T10:05:15.151362Z","iopub.status.idle":"2023-09-07T10:05:15.428303Z","shell.execute_reply.started":"2023-09-07T10:05:15.151332Z","shell.execute_reply":"2023-09-07T10:05:15.427162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-light\" role=\"alert\" style=\"border: 1px solid #6c757d; border-radius: 5px; padding: 10px;\">\n    <h1 align=\"center\">Text Generation</h1>\n</div>\nAs we've learned, this text model's primary task is to predict the next word in a sequence given a sequence. To generate a longer sentence, we'll need to run a loop to combine these words into a coherent sentence.","metadata":{}},{"cell_type":"code","source":"def test_generator(string, num):\n    if len(string)==0:\n        print(\"Error: No word found\")\n        return\n    for _ in range(num):\n        token_list = tokenizer.texts_to_sequences([string])[0]\n        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding = \"pre\")\n        probabilities = model.predict(token_list)\n        choice = np.random.choice([1,2,3])\n        predicted = np.argsort(probabilities, axis = -1)[0][-choice]\n        if predicted !=0:\n            generated_word = tokenizer.index_word[predicted]\n            string += \" \" + generated_word\n    print(string)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T10:05:15.430018Z","iopub.execute_input":"2023-09-07T10:05:15.430410Z","iopub.status.idle":"2023-09-07T10:05:15.439568Z","shell.execute_reply.started":"2023-09-07T10:05:15.430374Z","shell.execute_reply":"2023-09-07T10:05:15.438354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Moment of Truth\nNow, let's see what we've achieved so far!","metadata":{}},{"cell_type":"code","source":"test_generator(\"long live the king\", 20)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T10:05:15.441125Z","iopub.execute_input":"2023-09-07T10:05:15.441863Z","iopub.status.idle":"2023-09-07T10:05:20.437485Z","shell.execute_reply.started":"2023-09-07T10:05:15.441829Z","shell.execute_reply":"2023-09-07T10:05:20.436406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another try:","metadata":{}},{"cell_type":"code","source":"test_generator(\"Life\", 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-07T10:07:34.153585Z","iopub.execute_input":"2023-09-07T10:07:34.153953Z","iopub.status.idle":"2023-09-07T10:07:34.893934Z","shell.execute_reply.started":"2023-09-07T10:07:34.153924Z","shell.execute_reply":"2023-09-07T10:07:34.892894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Congratulations! You've learned how to create a text generator in TensorFlow from scratch!\n\n","metadata":{}}]}